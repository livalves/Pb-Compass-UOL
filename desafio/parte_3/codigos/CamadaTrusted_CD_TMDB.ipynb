{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfWiKKoEXJ_9"
      },
      "source": [
        "**Processamento da Trusted** - Desafio Parte III\n",
        "- Carga de dados respons√°vel pelo processamento dos arquivos JSON referente ao TMDB."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-PsNmFRgyAd"
      },
      "outputs": [],
      "source": [
        "!pip install boto3\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OA3AjJgazS3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Trusted-CD\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2BxuMlHgoHg"
      },
      "outputs": [],
      "source": [
        "aws_access_key_id = ''\n",
        "aws_secret_access_key = ''\n",
        "aws_session_token = ''\n",
        "\n",
        "client = boto3.client('s3', aws_access_key_id = aws_access_key_id,\n",
        "                  aws_secret_access_key = aws_secret_access_key,\n",
        "                  aws_session_token = aws_session_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av10vomCgdY6"
      },
      "outputs": [],
      "source": [
        "dfs = []\n",
        "\n",
        "bucket_name = \"data-lake-livia\"\n",
        "\n",
        "entry_keys = ['Ram/tmdb/json/2024/02/24/data_53-215317.json', 'Ram/tmdb/json/2024/02/24/data_53-215442.json', 'Ram/tmdb/json/2024/02/24/data_53-215524.json', 'Ram/tmdb/json/2024/02/24/data_53-215706.json']\n",
        "\n",
        "for aws_key in entry_keys:\n",
        "  s3_object = client.get_object(Bucket=bucket_name, Key= aws_key)\n",
        "  df_pd = pd.read_json(s3_object['Body'])\n",
        "  dfs.append(df_pd)\n",
        "\n",
        "df_pd = pd.concat(dfs, ignore_index=True)\n",
        "print(df_pd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2n7IvI-xBSu"
      },
      "outputs": [],
      "source": [
        "results = df_pd['results']\n",
        "df_results = pd.DataFrame(results.tolist())\n",
        "df_json = spark.createDataFrame(df_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QqyTiSUZX4q3"
      },
      "outputs": [],
      "source": [
        "df_json.show()\n",
        "\n",
        "df_json.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbFy486GgxYJ"
      },
      "outputs": [],
      "source": [
        "file_name = \"suspense.parquet\"\n",
        "\n",
        "file_movies = f'/content/{file_name}'\n",
        "df_json.coalesce(1).write.parquet(file_movies, mode=\"overwrite\")\n",
        "\n",
        "for filename in os.listdir(file_movies):\n",
        "    if filename.startswith('part-'):\n",
        "        new_filename = file_name\n",
        "        old_filepath = os.path.join(file_movies, filename)\n",
        "        new_filepath = os.path.join(file_movies, new_filename)\n",
        "        os.rename(old_filepath, new_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R1smXdJBozv"
      },
      "outputs": [],
      "source": [
        "folder_name = \"Suspense\"\n",
        "\n",
        "up_file_movies = f'/content/{file_name}/{file_name}'\n",
        "aws_file_movies = f\"TRT/{folder_name}/{file_name}\"\n",
        "\n",
        "try:\n",
        "    client.upload_file(up_file_movies, bucket_name, aws_file_movies)\n",
        "    print(f\"Sucesso! Arquivo parquet enviado para o S3.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro durante o upload: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
