{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfWiKKoEXJ_9"
      },
      "source": [
        "**Processamento da Refined** - Desafio Parte III\n",
        "- Processamento do arquivo parquet movies da camada Trusted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-PsNmFRgyAd"
      },
      "outputs": [],
      "source": [
        "!pip install boto3\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_OA3AjJgazS3"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Refined\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "y2BxuMlHgoHg"
      },
      "outputs": [],
      "source": [
        "aws_access_key_id = ''\n",
        "aws_secret_access_key = ''\n",
        "aws_session_token = ''\n",
        "\n",
        "client = boto3.client('s3', aws_access_key_id = aws_access_key_id,\n",
        "                  aws_secret_access_key = aws_secret_access_key,\n",
        "                  aws_session_token = aws_session_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av10vomCgdY6"
      },
      "outputs": [],
      "source": [
        "from io import BytesIO\n",
        "\n",
        "bucket_name = \"data-lake-livia\"\n",
        "input_key = \"TRT/Movies/movies.parquet\"\n",
        "\n",
        "s3_object = client.get_object(Bucket=bucket_name, Key=input_key)['Body'].read()\n",
        "parquet = BytesIO(s3_object)\n",
        "df_pd = pd.read_parquet(parquet)\n",
        "\n",
        "print(df_pd.info())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IPe4HWQq1WEo"
      },
      "outputs": [],
      "source": [
        "colunas_selecionadas = ['id', 'tituloPincipal', 'anoLancamento', 'genero', 'notaMedia', 'numeroVotos']\n",
        "df_selecionado = df_pd[colunas_selecionadas]\n",
        "\n",
        "df_suspense = df_selecionado[df_selecionado['genero'].apply(lambda x: 'Thriller' in x)]\n",
        "\n",
        "print(df_suspense)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "WbFy486GgxYJ"
      },
      "outputs": [],
      "source": [
        "file_name = \"suspense.parquet\"\n",
        "\n",
        "file_movies = f'/content/{file_name}'\n",
        "df_suspense.to_parquet(file_movies, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R1smXdJBozv"
      },
      "outputs": [],
      "source": [
        "folder_name = \"Suspense\"\n",
        "\n",
        "up_file_movies = f'/content/{file_name}'\n",
        "aws_file_movies = f\"RFD/{folder_name}/{file_name}\"\n",
        "\n",
        "try:\n",
        "    client.upload_file(up_file_movies, bucket_name, aws_file_movies)\n",
        "    print(f\"Sucesso! Arquivo parquet enviado para o S3.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro durante o upload: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
