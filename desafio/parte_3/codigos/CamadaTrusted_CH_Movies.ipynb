{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfWiKKoEXJ_9"
      },
      "source": [
        "**Processamento da Trusted** - Desafio Parte III\n",
        "- Carga histórica responsável pelo processamento dos arquivos CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-PsNmFRgyAd"
      },
      "outputs": [],
      "source": [
        "!pip install boto3\n",
        "!pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_OA3AjJgazS3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql.session import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Trusted-CH\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2BxuMlHgoHg"
      },
      "outputs": [],
      "source": [
        "aws_access_key_id = ''\n",
        "aws_secret_access_key = ''\n",
        "aws_session_token = ''\n",
        "\n",
        "client = boto3.client('s3', aws_access_key_id = aws_access_key_id,\n",
        "                  aws_secret_access_key = aws_secret_access_key,\n",
        "                  aws_session_token = aws_session_token)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av10vomCgdY6"
      },
      "outputs": [],
      "source": [
        "bucket_name = \"data-lake-livia\"\n",
        "input_csv_key = \"Ram/Local/CSV/Movies/2024/03/04/movies.csv\"\n",
        "\n",
        "s3_object = client.get_object(Bucket=bucket_name, Key=input_csv_key)\n",
        "df_pd = pd.read_csv(s3_object['Body'], sep=\"|\", encoding=\"utf-8\")\n",
        "print(df_pd.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r3W4b7zPdW5g"
      },
      "outputs": [],
      "source": [
        "df_pd.replace(r'\\N', '', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2n7IvI-xBSu"
      },
      "outputs": [],
      "source": [
        "df_csv = spark.createDataFrame(df_pd)\n",
        "df_csv.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XjkfDhUvZHg"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.types import StringType, ArrayType\n",
        "from pyspark.sql.functions import col, split\n",
        "df = df_csv\\\n",
        "    .withColumn(\"genero\", split(col(\"genero\"), \",\").cast(ArrayType(StringType())))\\\n",
        "    .withColumn(\"profissao\", split(col(\"profissao\"), \",\").cast(ArrayType(StringType())))\\\n",
        "    .withColumn(\"titulosMaisConhecidos\", split(col(\"titulosMaisConhecidos\"), \",\").cast(ArrayType(StringType())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aSTqSvjF0PKA"
      },
      "outputs": [],
      "source": [
        "df.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbFy486GgxYJ"
      },
      "outputs": [],
      "source": [
        "file_name = \"movies.parquet\"\n",
        "\n",
        "file_movies = f'/content/{file_name}'\n",
        "df.coalesce(1).write.parquet(file_movies, mode=\"overwrite\")\n",
        "\n",
        "for filename in os.listdir(file_movies):\n",
        "    if filename.startswith('part-'):\n",
        "        new_filename = file_name\n",
        "        old_filepath = os.path.join(file_movies, filename)\n",
        "        new_filepath = os.path.join(file_movies, new_filename)\n",
        "        os.rename(old_filepath, new_filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R1smXdJBozv"
      },
      "outputs": [],
      "source": [
        "folder_name = \"Movies\"\n",
        "\n",
        "up_file_movies = f'/content/{file_name}/{file_name}'\n",
        "aws_file_movies = f\"TRT/{folder_name}/{file_name}\"\n",
        "\n",
        "try:\n",
        "    client.upload_file(up_file_movies, bucket_name, aws_file_movies)\n",
        "    print(f\"Sucesso! Arquivo parquet enviado para o S3.\")\n",
        "except Exception as e:\n",
        "    print(f\"Erro durante o upload: {e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
